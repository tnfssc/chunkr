fastapi
packaging
wheel
huggingface_hub
torch==2.2.0
# torchvision==0.17.0  # Find a compatible version for torch 2.3.1
# autoawq==0.2.6
# flash-attn==2.5.7
vllm==0.6.1
numpy==1.24.4
Pillow==10.3.0
requests==2.31.0
torchvision==0.18.0
git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830
accelerate
huggingface_hub
fastapi
pydantic
uvicorn
python-multipart
PIL
qwen_vl_utils

